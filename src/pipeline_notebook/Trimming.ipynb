{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality trimming of raw FASTQ files\n",
    "This notebook is for quality trimming of raw fastq files. The raw files are expected to be already uploaded in the Hopsworks HDFS. It applies `trimmomatic` and `cutadapt` tools. For trimmomatic, both single and paired end mode is used according to the sample name. (if sample name contains `_R` its treated as paired file else as single ended). For cutadapt processing only **paired R2** files are used as input. Different arguments can be set in `settings.yml` file.\n",
    "\n",
    "Note: Currently only RNA sequence processing is supported via  value of flag `IS_RNA` set to `True`. DNA sequencing is not yet supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import utils\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_full=utils.load_arguments(sys.argv)\n",
    "\n",
    "OUTPUT_DATASET=args_full['OUTPUT_DATASET']\n",
    "INPUT_ROOT_PATH=args_full['INPUT_ROOT_PATH']\n",
    "RUN_FOLDER=args_full['RUN_FOLDER']\n",
    "\n",
    "args=args_full[utils.KEY_TRIMMOMATIC]\n",
    "\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    INPUT_ROOT=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "   INPUT_ROOT=args['INPUT_ROOT']\n",
    "\n",
    "\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    WORK_PATH=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "\n",
    "OUTPUT_PAIRED=os.path.join(WORK_PATH,args['OUTPUT_PAIRED'])\n",
    "OUTPUT_UNPAIRED=os.path.join(WORK_PATH,args['OUTPUT_UNPAIRED'])\n",
    "OUTPUT_SINGLE=os.path.join(WORK_PATH,args['OUTPUT_SINGLE'])\n",
    "\n",
    "IS_SAVE_UNPAIRED= args['SAVE_UNPAIRED']\n",
    "\n",
    "USE_CUTADAPT = args['USE_CUTADAPT']\n",
    "IS_RNA = args['IS_RNA']\n",
    "LOGS_ROOT = args['LOGS_ROOT']\n",
    "PHRED = args['PHRED']\n",
    "LEADING = str(args['LEADING'])\n",
    "TRAILING = str(args['TRAILING'])\n",
    "SLIDING_WINDOW = str(args['SLIDINGWINDOW'])\n",
    "MIN_LEN = str(args['MINLEN'])\n",
    "THREADS = args['THREADS']\n",
    "JAVA = \"java -jar\"\n",
    "CUTADPAT_ARGS='cutadapt -j 0 -u 3 -o'\n",
    "SPACE=utils.SPACE\n",
    "\n",
    "\n",
    "#### Get trimmomatic jar and adapter files\n",
    "JAR_PATH=args['JAR']\n",
    "ADAPTER_PAIR_PATH=args['ADAPTER_PAIR']\n",
    "ADAPTER_SINGLE_PATH=args['ADAPTER_SINGLE']\n",
    "if JAR_PATH is None:\n",
    "    sys.exit(utils.TRIMMOMATIC_NOT_FOUND)\n",
    "\n",
    "if (ADAPTER_PAIR_PATH or ADAPTER_SINGLE_PATH) is None:\n",
    "    sys.exit(utils.TRIMMOMATIC_ADAPTER_NOT_FOUND)\n",
    "\n",
    "tool=os.path.basename(JAR_PATH)\n",
    "ADAPTER_PAIR=os.path.basename(ADAPTER_PAIR_PATH)\n",
    "ADAPTER_SINGLE=os.path.basename(ADAPTER_SINGLE_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cut(input_file,log_file):\n",
    "    \"\"\"\n",
    "    cutadpat\n",
    "    \"\"\"\n",
    "    \n",
    "    out_trim='cut_'+input_file\n",
    "    params={out_trim : input_file}\n",
    "    cmd_cut=utils.build_command(CUTADPAT_ARGS, params)\n",
    "    with open(log_file, \"a\") as f:\n",
    "        subprocess.run(cmd_cut.split(' '),stdout=f,stderr=f)\n",
    "    if out_trim:\n",
    "        os.remove(input_file)\n",
    "        os.rename(out_trim,input_file)\n",
    "        \n",
    "    params.clear()\n",
    "\n",
    "    \n",
    "\n",
    "def apply_trim_single(file_input):\n",
    "    \"\"\"\n",
    "    Trimmomatic on single end file.\n",
    "    Output files are copied back to hdfs\n",
    "    :param file_input:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # get file name\n",
    "    file_name=os.path.basename(file_input)\n",
    "    file_output='trim_'+file_name\n",
    "\n",
    "    # check if output already exists\n",
    "    if  utils.skip_file(file_name,file_output,OUTPUT_SINGLE):\n",
    "        return [-1]\n",
    "\n",
    "    # copy input to local\n",
    "    hdfs.copy_to_local(file_input, overwrite=True)\n",
    "    if not (os.path.exists(tool)):\n",
    "        hdfs.copy_to_local(JAR_PATH)\n",
    "    if not (os.path.exists(ADAPTER_SINGLE)):\n",
    "        hdfs.copy_to_local(ADAPTER_SINGLE_PATH)\n",
    "\n",
    "    # single end attributes\n",
    "    attribute='SE -'+PHRED\n",
    "    threads='-threads '+str(THREADS)\n",
    "    illuminaclip_adapters = \"ILLUMINACLIP:\"+ADAPTER_SINGLE+\":2:30:10\"\n",
    "    illuminaclip_Attribute = \"LEADING:\"+LEADING+SPACE+\"TRAILING:\"+TRAILING+SPACE+\"SLIDINGWINDOW:\"+SLIDING_WINDOW+SPACE+\"MINLEN:\"+MIN_LEN\n",
    "    s=SPACE\n",
    "    # command to run\n",
    "    cmd_single =JAVA + s + tool + s + attribute + s + threads + s + file_name + s + file_output + s + illuminaclip_adapters + s + illuminaclip_Attribute\n",
    "    print('INFO: Run trimmomatic command: ', cmd_single)\n",
    "    # run\n",
    "    log_file=utils.get_sampleName_with_lane(file_name)+'.txt'\n",
    "    # run\n",
    "    try:\n",
    "        with open(log_file, \"w\") as f:\n",
    "            result=subprocess.run(cmd_single.split(),stdout=f,stderr=f,check=True)\n",
    "\n",
    "        if result.returncode==0 and os.path.exists(file_output) :\n",
    "            # copy output to hdfs\n",
    "            hdfs.copy_to_hdfs(file_output, OUTPUT_SINGLE, overwrite=True)\n",
    "            # remove local files\n",
    "            os.remove(file_output)\n",
    "\n",
    "        # copy logs\n",
    "        hdfs.copy_to_hdfs(log_file, LOGS_ROOT, overwrite=True)\n",
    "        return [True,file_output]\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    except IOError:\n",
    "        traceback.print_exc()\n",
    "        utils.hdfs_delete_file(os.path.join(OUTPUT_SINGLE,file_output))\n",
    "        return False\n",
    "    finally:\n",
    "        if os.path.exists(log_file):\n",
    "            os.remove(log_file)\n",
    "        os.remove(file_name)\n",
    "\n",
    "\n",
    "def apply_trim_paired(x) :\n",
    "    \"\"\"\n",
    "    Trimmomatic on paired end file via subprocess.\n",
    "    Output files are copied to hdfs.\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    r1=x[0] # R1\n",
    "    r2=x[1] # R2\n",
    "\n",
    "    # get file names\n",
    "    filename_forward=os.path.basename(r1)\n",
    "    filename_reverse=os.path.basename(r2)\n",
    "    # append suffixes to output\n",
    "    output_forward_paired =utils.TRIM_PAIRED+filename_forward\n",
    "    output_forward_unpaired=utils.TRIM_UNPAIRED+filename_forward\n",
    "    output_reverse_paired =utils.TRIM_PAIRED+filename_reverse\n",
    "    output_reverse_unpaired=utils.TRIM_UNPAIRED+filename_reverse\n",
    "    log_file=utils.get_sampleName_with_lane(filename_forward)+'.txt'\n",
    "    try:\n",
    "        # check if output already exists\n",
    "        if  utils.skip_file(filename_forward,output_forward_paired,OUTPUT_PAIRED):\n",
    "            return [-1]\n",
    "\n",
    "        hdfs.copy_to_local(r1, overwrite=True)\n",
    "        hdfs.copy_to_local(r2, overwrite=True)\n",
    "        if not (os.path.exists(tool)):\n",
    "            hdfs.copy_to_local(JAR_PATH)\n",
    "        if not (os.path.exists(ADAPTER_PAIR)):\n",
    "            hdfs.copy_to_local(ADAPTER_PAIR_PATH)\n",
    "\n",
    "        ### paired end attributes\n",
    "        attribute='PE -'+PHRED\n",
    "        threads='-threads '+str(THREADS)\n",
    "        illuminaclip_adapters = \"ILLUMINACLIP:\"+ADAPTER_PAIR+\":2:30:10:2:keepBothReads\"\n",
    "        illuminaclip_Attribute = \"LEADING:\"+LEADING+SPACE+\"TRAILING:\"+TRAILING+SPACE+\"SLIDINGWINDOW:\"+SLIDING_WINDOW+SPACE+\"MINLEN:\"+MIN_LEN\n",
    "\n",
    "        s=SPACE\n",
    "        cmd1 = JAVA + s + tool + s + attribute + s + threads + s + filename_forward + s + filename_reverse + s + output_forward_paired + s + output_forward_unpaired\n",
    "        cmd2 = s + output_reverse_paired + s + output_reverse_unpaired + s + illuminaclip_adapters + s + illuminaclip_Attribute\n",
    "        # final command\n",
    "        cmd_paired = cmd1 + cmd2\n",
    "        print('INFO: Run trimmomatic command: ', cmd_paired)\n",
    "        # run\n",
    "        with open(log_file, \"w\") as f:\n",
    "            result=subprocess.run(cmd_paired.split(),stdout=f,stderr=f,check=True)\n",
    "\n",
    "        if result.returncode==0:\n",
    "            if USE_CUTADAPT: # run cutadapt on R2\n",
    "                cut(output_reverse_paired,log_file)\n",
    "\n",
    "            # copy output to hdfs\n",
    "            hdfs.copy_to_hdfs(output_forward_paired, OUTPUT_PAIRED, overwrite=True)\n",
    "            hdfs.copy_to_hdfs(output_reverse_paired, OUTPUT_PAIRED, overwrite=True)\n",
    "            if IS_SAVE_UNPAIRED:\n",
    "                hdfs.copy_to_hdfs(output_forward_unpaired, OUTPUT_UNPAIRED, overwrite=True)\n",
    "                hdfs.copy_to_hdfs(output_reverse_unpaired, OUTPUT_UNPAIRED, overwrite=True)\n",
    "\n",
    "        # copy logs\n",
    "        hdfs.copy_to_hdfs(log_file, LOGS_ROOT, overwrite=True)\n",
    "        # remove local files\n",
    "        os.remove(output_reverse_paired)\n",
    "        os.remove(output_forward_unpaired)\n",
    "        os.remove(output_forward_paired)\n",
    "        os.remove(output_reverse_unpaired)\n",
    "\n",
    "        return [True,output_forward_paired,output_forward_unpaired,output_reverse_paired,output_reverse_unpaired]\n",
    "    except subprocess.CalledProcessError:\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    except IOError:\n",
    "        traceback.print_exc()\n",
    "        utils.hdfs_delete_file(os.path.join(OUTPUT_PAIRED,output_forward_paired))\n",
    "        utils.hdfs_delete_file(os.path.join(OUTPUT_PAIRED,output_reverse_paired))\n",
    "        return False\n",
    "    finally:\n",
    "        if os.path.exists(log_file):\n",
    "            os.remove(log_file)\n",
    "        if os.path.exists(filename_forward):\n",
    "            os.remove(filename_forward)\n",
    "        if os.path.exists(filename_reverse):\n",
    "            os.remove(filename_reverse)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input files hdfs path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files=utils.load_file_names(INPUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of all single end files and run trimmomatic in single mode in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### single\n",
    "single_files=[f for f in all_files if utils.R_IDENTIFIER not in f]\n",
    "\n",
    "print('number of single input  files processing ', len(single_files))\n",
    "dataRdd=sc.parallelize(single_files,sc.getConf().get(\"spark.executor.instances\"))\n",
    "\n",
    "# run\n",
    "trimmedSingleFiles=dataRdd.map(lambda x: apply_trim_single(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair R1 and R2 as a tuple in a list and run trimmomatic in paired end in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pair R1 and R2\n",
    "pairedList =  utils.group_R1R2(all_files)\n",
    "print('number of input paired files processing ', len(pairedList))\n",
    "dataPairedRdd=sc.parallelize(pairedList,sc.getConf().get(\"spark.executor.instances\"))\n",
    "# run\n",
    "trimmedFiles=dataPairedRdd.map(lambda x: apply_trim_paired(x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-fd840124",
   "language": "python",
   "display_name": "PyCharm (ki-pipeline)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}