{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kraken2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "import pysam\n",
    "import os\n",
    "from hops import hdfs\n",
    "import utils\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import subprocess\n",
    "import stat\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_full=utils.load_arguments(sys.argv)\n",
    "\n",
    "OUTPUT_DATASET=args_full[utils.OUTPUT_DATASET]\n",
    "INPUT_ROOT_PATH=args_full[utils.INPUT_ROOT_PATH]\n",
    "RUN_FOLDER=args_full[utils.RUN_FOLDER]\n",
    "WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "args=args_full['Kraken']\n",
    "\n",
    "\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    inputRoot=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "    inputRoot=os.path.join(WORK_PATH,args['INPUT_ROOT'])\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    outputRoot=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    outputRoot=os.path.join(WORK_PATH,args['OUTPUT_ROOT'])\n",
    "\n",
    "\n",
    "kraken_path=args['KRAKEN_PATH']\n",
    "tool=os.path.basename(kraken_path)\n",
    "kk_db_path=args['KRAKEN_DB_PATH']\n",
    "is_save_all_outputs=args['SAVE_FULL_OUTPUT']\n",
    "threads=args['THREADS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install kraken from hdfs source\n",
    "def load_kraken(kraken_path):\n",
    "    tool=os.path.basename(kraken_path)\n",
    "    \n",
    "    hdfs.copy_to_local(kraken_path)\n",
    "\n",
    "    st = os.stat(tool+'/kraken2')\n",
    "    os.chmod(tool+'/kraken2', st.st_mode | stat.S_IEXEC)\n",
    "    \n",
    "    st = os.stat(tool+'/classify')\n",
    "    os.chmod(tool+'/classify', st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "\n",
    "\n",
    "def compress_file(file):\n",
    "    compress_file=file+'.gz'\n",
    "    with open(file, 'rb') as f_in:        \n",
    "        with gzip.open(compress_file, 'wb',compresslevel=1) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "            \n",
    "    return compress_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def apply_kraken_single(file_path,kk_db_path):\n",
    "    \"\"\"\n",
    "    Runs kraken on single file via subprcess.\n",
    "    First kraken is installed by copying kraken tool from hdfs.\n",
    "    Outputs are copied back to hdfs.\n",
    "    If an output file name is already present in output directory the processing\n",
    "    of file is skipped to avoid processing of same file in case of resubmit of failed run.\n",
    "\n",
    "    :param file_path:\n",
    "    :param kk_db_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    file=os.path.split(file_path)[1]  \n",
    "    sample=os.path.splitext(os.path.splitext(file)[0])[0]    \n",
    "    report=sample+'_report.txt'\n",
    "    \n",
    "    if not hdfs.exists(os.path.join(outputRoot,'report',report)): # check if output already exists\n",
    "    \n",
    "        load_kraken(kraken_path) # install kraken\n",
    "\n",
    "        kk_db=os.path.split(kk_db_path)[1]  \n",
    "        if not os.path.exists(kk_db):\n",
    "            hdfs.copy_to_local(kk_db_path)\n",
    "\n",
    "        hdfs.copy_to_local(file_path)\n",
    "\n",
    "        output=sample+'_out.txt'\n",
    "        unclassified=sample+'_unclassified.txt'\n",
    "        if is_save_all_outputs: # save unclassified and output files\n",
    "            params={'--db':kk_db,'--threads': threads, '--report': report,'--report-minimizer-data':'','--report-zero-counts':'','--unclassified-out': unclassified, file: '','--output': output }\n",
    "        else :\n",
    "            params={'--db':kk_db,'--threads': threads, '--report': report,'--report-minimizer-data':'','--report-zero-counts':'','--unclassified-out': '/dev/null', file: '','--output': '/dev/null' }\n",
    "        cmd=utils.build_command(tool+'/kraken2',params)\n",
    "        print(cmd)\n",
    "        try:\n",
    "            status=subprocess.run(cmd.split(),stdout=subprocess.PIPE,check=True)\n",
    "\n",
    "            if status.returncode==0 and os.path.exists(report):\n",
    "                hdfs.copy_to_hdfs(report,os.path.join(outputRoot,'report'),overwrite=True)\n",
    "                os.remove(report)\n",
    "\n",
    "                if is_save_all_outputs:\n",
    "                    # compress\n",
    "                    c_output=compress_file(output)\n",
    "                    c_unclassified=compress_file(unclassified)\n",
    "                    # copy to hdfs\n",
    "                    hdfs.copy_to_hdfs(c_unclassified,os.path.join(outputRoot,'unclassified'),overwrite=True)\n",
    "                    hdfs.copy_to_hdfs(c_output,os.path.join(outputRoot,'output'),overwrite=True)\n",
    "                    # remove local files\n",
    "                    os.remove(output)\n",
    "                    os.remove(unclassified)\n",
    "                    os.remove(c_output)\n",
    "                    os.remove(c_unclassified)\n",
    "\n",
    "\n",
    "            return file\n",
    "        except subprocess.CalledProcessError:\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        finally:\n",
    "             os.remove(file)\n",
    "    else :\n",
    "        print('skipping existing file: ', file)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all input file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inputFiles=utils.load_file_names(inputRoot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize\n",
    "rdd=sc.parallelize(inputFiles,sc.getConf().get(\"spark.executor.instances\"))\n",
    "# run\n",
    "final=rdd.map( lambda x: apply_kraken_single(x,kk_db_path) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}