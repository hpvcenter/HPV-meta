{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and Convert to FASTQ\n",
    "\n",
    "Sorts a BAM file and converts to FASTQ.\n",
    "The input file is first checked for paired reads count. If its non zero its treated as a paired-end file.\n",
    "Based on the user flag to split into R1 and R2 the paired file is separated into two fastq file. Else a single fastq file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import pysam\n",
    "from hops import hdfs\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args_full=utils.load_arguments(sys.argv)\n",
    "\n",
    "OUTPUT_DATASET=args_full[utils.OUTPUT_DATASET]\n",
    "INPUT_ROOT_PATH=args_full[utils.INPUT_ROOT_PATH]\n",
    "RUN_FOLDER=args_full[utils.RUN_FOLDER]\n",
    "WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "args=args_full[utils.KEY_SORTCONVERT]\n",
    "\n",
    "\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    inputRoot=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "    inputRoot=os.path.join(WORK_PATH,args['INPUT_ROOT'])\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    OUTPUT_FASTQ=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    OUTPUT_FASTQ=os.path.join(WORK_PATH,args['OUTPUT_FASTQ'])\n",
    "\n",
    "\n",
    "THREADS=str(args['THREADS'])\n",
    "SPACE=utils.SPACE\n",
    "# split separate R1 and R2 if paired file\n",
    "is_split_R1R2=args['SPLIT_PAIRS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_paired_reads(x):\n",
    "    \"\"\"\n",
    "    Count paired  in BAM reads countfile using pysam.\n",
    "    Returns True if non zero.\n",
    "    \"\"\"\n",
    "\n",
    "    o=pysam.view(x, '-c','-f','1',catch_stdout=True)\n",
    "    if int(o)!=0: # if non zero paired reads count\n",
    "        return True\n",
    "\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def isPaired(file):\n",
    "    \"\"\"\n",
    "    Counts paired reads count in BAM using samtools.\n",
    "    Returns True if non zero.\n",
    "    \"\"\"\n",
    "\n",
    "    cmd1=utils.SAMTOOLS+' view -c -f 1 '+file \n",
    "    print(cmd1)\n",
    "    p1=subprocess.Popen(cmd1.split(),stdout=subprocess.PIPE)\n",
    "    cmd2='head -n 1'\n",
    "    p2=subprocess.run(cmd2.split(),stdin=p1.stdout,stdout=subprocess.PIPE)\n",
    "    if p1.stdout  :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def sort(file):\n",
    "    \"\"\"\n",
    "    Sort given BAM file using pysam\n",
    "    \"\"\"\n",
    "    sort_file = utils.SORTED_PREFIX+file\n",
    "    try:\n",
    "        pysam.sort('-@',THREADS,'-m','2G','-n',file,'-o',sort_file,catch_stdout=False)\n",
    "    except pysam.SamtoolsError:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return sort_file\n",
    "\n",
    "\n",
    "\n",
    "def sort_convert_fastq(file_path):\n",
    "    \"\"\"\n",
    "    Map function to run on single BAM file.\n",
    "    First the file is sorted and then the sorted file is converted to\n",
    "    single or paired FASTQ files depending on user argument\n",
    "    and if file is paired or not.\n",
    "    Output is copied back to hdfs.\n",
    "    If output file name is already present in destination the process is skipped\n",
    "    to avoid processing of same file incase of resubmit of failed run.\n",
    "\n",
    "\n",
    "    P.S/TODO: Commented code is to run via samtools and stays due to hadoop18 problem\n",
    "    of running samtools via subprocess.\n",
    "    Once hadoop18 is fixed samtools can be uncommented and pysam code then can be subsequently deleted\n",
    "    \"\"\"\n",
    "    file=os.path.split(file_path)[1]\n",
    "    filename=os.path.splitext(file)[0]      \n",
    "    output=filename+'.fq.gz'\n",
    "    if not hdfs.exists(os.path.join(OUTPUT_FASTQ,output)): # check if output already exists\n",
    "               \n",
    "    \n",
    "        hdfs.copy_to_local(file_path, overwrite=True)\n",
    "         \n",
    "        out_file_r1=''\n",
    "        out_file_r2=''\n",
    "        out_file=''\n",
    "\n",
    "         # check if bam file has paired reads\n",
    "        try:\n",
    "\n",
    "            isPairedFile=count_paired_reads(file)\n",
    "\n",
    "\n",
    "            print('is paired ? ', isPairedFile)\n",
    "\n",
    "            # sort\n",
    "            print(\"INFO: Run sort \", file)\n",
    "            sort_file=sort(file)\n",
    "            os.remove(file)\n",
    "            os.rename(sort_file,sort_file.split(utils.SORTED_PREFIX)[1])\n",
    "\n",
    "            # if the bam file is paired and user wants to split into separate R1 and R2 fastq\n",
    "            if (isPairedFile and is_split_R1R2):\n",
    "                out_file_r1=filename+utils.R1_SUFFIX_EXTENSION\n",
    "                out_file_r2=filename+utils.R2_SUFFIX_EXTENSION\n",
    "                #params={'fastq':file,'-1':out_file_r1,'-2':out_file_r2, '-0':'/dev/null', '-s':'/dev/null', '-c':1 ,'-@':THREADS}\n",
    "                try:\n",
    "                    pysam.fastq(file,'-1',out_file_r1,'-2',out_file_r2, '-0','/dev/null', '-s','/dev/null', '-c','1','-@',THREADS,'-N',catch_stdout=False)\n",
    "                except pysam.SamtoolsError:\n",
    "                    traceback.print_exc()\n",
    "\n",
    "            else:   # else if the file is single read or the user wants to split into a single fastq\n",
    "                out_file=filename+'.fq.gz'\n",
    "                #params={'fastq': file,'-0': out_file, '-s':'/dev/null', '-c':1, '-@': THREADS}\n",
    "                if isPairedFile: # samtools fastq has two different argument values for output if the file is paired or not\n",
    "                    OUT_ARG='-o'\n",
    "                else :\n",
    "                    OUT_ARG='-0'\n",
    "                try:\n",
    "                    pysam.fastq(file, OUT_ARG, out_file,  '-c','1','-@',THREADS,'-N',catch_stdout=False)\n",
    "                except pysam.SamtoolsError:\n",
    "                    traceback.print_exc()\n",
    "\n",
    "        # convert to fastq command  using samtools\n",
    "        #     cmdConvert=utils.build_command(utils.SAMTOOLS,params)\n",
    "        #     print('INFO: Run sort and convert', cmdConvert)\n",
    "        #     p1 = subprocess.run(cmdConvert.split(), stdout=subprocess.PIPE) # run convert from pipe from sort command\n",
    "\n",
    "            os.remove(file)\n",
    "            if os.path.exists(out_file): # either a single fastq exists or separate R1 R2 fastq files\n",
    "                hdfs.copy_to_hdfs(out_file,OUTPUT_FASTQ,overwrite=True)\n",
    "                os.remove(out_file)\n",
    "                return True\n",
    "            if os.path.exists(out_file_r1):\n",
    "                hdfs.copy_to_hdfs(out_file_r1,OUTPUT_FASTQ,overwrite=True)\n",
    "                hdfs.copy_to_hdfs(out_file_r2,OUTPUT_FASTQ,overwrite=True)\n",
    "                os.remove(out_file_r1)\n",
    "                os.remove(out_file_r2)\n",
    "                return True\n",
    "        except pysam.SamtoolsError:\n",
    "            traceback.print_exc()\n",
    "            utils.hdfs_delete_file(file_path) # delete corrupted input file\n",
    "            return False\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('skipping existing file: ', filename)            \n",
    "        return None\n",
    "\n",
    "        \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input files hdfs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFiles=utils.load_file_names(inputRoot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "finalList = sc.parallelize(inputFiles,sc.getConf().get(\"spark.executor.instances\")).map(sort_convert_fastq).collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}