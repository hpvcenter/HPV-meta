{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>606</td><td>application_1615491195715_0807</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1615491195715_0807/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop4:8042/node/containerlogs/container_e41_1615491195715_0807_01_000001/TCGA_viruses__dhananja\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from collections import Counter,OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pandas\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args_full=utils.load_arguments(sys.argv)\n",
    "args=args_full[utils.KEY_FILTER_DIAMOND]\n",
    "\n",
    "OUTPUT_DATASET=args_full[utils.OUTPUT_DATASET]\n",
    "INPUT_ROOT_PATH=args_full[utils.INPUT_ROOT_PATH]\n",
    "RUN_FOLDER=args_full[utils.RUN_FOLDER]\n",
    "WORK_PATH=os.path.join(OUTPUT_DATASET, RUN_FOLDER)\n",
    "\n",
    "REPORT_FILE=args['REPORT_FILE']\n",
    "FILTERED_PREFIX='_filtered'\n",
    "# check of input and output root override\n",
    "if args_full.get(utils.INPUT_OVERRIDE):\n",
    "    INPUT_ROOT=args_full.get(utils.INPUT_OVERRIDE)\n",
    "else :\n",
    "    INPUT_ROOT=os.path.join(WORK_PATH,args['INPUT_ROOT'])\n",
    "if args_full.get(utils.OUTPUT_OVERRIDE):\n",
    "    OUTPUT_ROOT=args_full.get(utils.OUTPUT_OVERRIDE)\n",
    "else:\n",
    "    OUTPUT_ROOT=os.path.join(WORK_PATH,args['OUTPUT_ROOT'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input files  9759"
     ]
    }
   ],
   "source": [
    "all_files=utils.load_file_names(INPUT_ROOT)\n",
    "\n",
    "print('Number of input files at input path', len(all_files) , INPUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OrderedDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-5fec35c0d304>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;31m# Dict (header, rowData)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mheaderRowData\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mOrderedDict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# type: Dict[str, str] #\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0mheaderRowData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mHDR_FILE_NAME\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0mheaderRowData\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mHDR_SAMPLE_NAME\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'OrderedDict' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" header variables \"\"\"\n",
    "HDR_FILE_NAME=\"File Name\"\n",
    "HDR_SAMPLE_NAME=\"Sample Name\"\n",
    "HDR_TOTAL_READS=\"Total Reads\"\n",
    "HDR_UNIQUE_READS=\"Unique Reads\"\n",
    "HDR_UNIQUE_PAIR_ENDS=\"Unique pairs\"\n",
    "HDR_HPV_COUNT_UNIQUE=\"HPV Type-Reads Unique Counts\"\n",
    "HDR_HPV_COUNT_EXCLUSIVE=\"HPV Type-Reads Exclusive Counts\"\n",
    "HDR_HPV_TYPES_NONOVERLAP=\"HPV Type-Reads Most Abundant\"\n",
    "HDR_HPV_TYPE_COV_PAIR=\"HPV Coverage-Pair\"\n",
    "HDR_COV_PER_SUBTYPE=\"Coverage per SubjectType\"\n",
    "HDR_POVSITIVE=\"Positive\"\n",
    "\n",
    "# Dict (header, rowData)\n",
    "headerRowData=OrderedDict() # type: Dict[str, str]\n",
    "# initliase headers in following order\n",
    "headerRowData[HDR_FILE_NAME]=\"\"\n",
    "headerRowData[HDR_SAMPLE_NAME]=\"\"\n",
    "headerRowData[HDR_TOTAL_READS]=\"\"\n",
    "headerRowData[HDR_UNIQUE_READS]=\"\"\n",
    "headerRowData[HDR_UNIQUE_PAIR_ENDS]=\"\"\n",
    "headerRowData[HDR_HPV_COUNT_UNIQUE]=\"\"\n",
    "headerRowData[HDR_HPV_COUNT_EXCLUSIVE]=\"\"\n",
    "headerRowData[HDR_HPV_TYPES_NONOVERLAP]=\"\"\n",
    "headerRowData[HDR_HPV_TYPE_COV_PAIR]=\"\"\n",
    "headerRowData[HDR_COV_PER_SUBTYPE]=\"\"\n",
    "headerRowData[HDR_POVSITIVE]=\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def table_collect_stats(file_path):\n",
    "    \"\"\"\n",
    "    Table to create statistics on 1 diamond report file.\n",
    "    The output is used to write into a file\n",
    "    :param String:diamond report hdfs file_path :\n",
    "    :return: List: with each item corresponding to 1 row for report\n",
    "    \"\"\"\n",
    "\n",
    "    subjects_reads_exclusive_full=''\n",
    "    subjects_reads_uniqueNonOverlap_full=''\n",
    "    subjects_reads_unique_full=''\n",
    "    subjects_cov_len=''\n",
    "    subject_unique_reads={} #  {subjectType: uniqueReadsCount}\n",
    "    coverage_per_sub_type={} # {subjectType: totalCoverageLen}\n",
    "    is_positive=False\n",
    "    subject_total_cov_len=''\n",
    "\n",
    "    file_path=hdfs.get_plain_path(file_path)\n",
    "    file_name=os.path.basename(file_path)\n",
    "\n",
    "    print('Processing file: ',  file_path)\n",
    "    try :\n",
    "        if FILTERED_PREFIX in file_path: # old job of filtering diamond positives added this prefix. Otherwise not needed\n",
    "            df=pandas.read_csv(file_path, sep='\\t', names=[\"Query\", \"Subject\", \"Percentage\", \"Alignment\",'Mismatch','Gap','Start','End','SA','EA','EV','BitScore'],header=1)\n",
    "        else :\n",
    "            df=pandas.read_csv(file_path, sep='\\t', names=[\"Query\", \"Subject\", \"Percentage\", \"Alignment\",'Mismatch','Gap','Start','End','SA','EA','EV','BitScore'],header=None)\n",
    "\n",
    "        totalRows=df.shape[0]\n",
    "        queries=df.Query\n",
    "        subjects=df.Subject\n",
    "        unique_queries=queries.unique()\n",
    "        unique_queries_set=set(unique_queries)\n",
    "        uniqueQueries_set_reducing=set(unique_queries.copy()) # used in calculate non overlapping reads\n",
    "        unique_count=len(unique_queries)  # unique reads\n",
    "         # subject types\n",
    "        subjects_filtered = get_subject_types(subjects)\n",
    "        df['SubjectType']=subjects_filtered\n",
    "        # protein type\n",
    "        protein_filtered=[ x.split('|')[1].split('.')[1] for x in subjects ]\n",
    "        df['ProteinType']=protein_filtered\n",
    "        # unique subject types\n",
    "        groupedQuerySubject=df[['Query','SubjectType']].groupby('SubjectType',sort=False).count().sort_values(by=['Query'],ascending=False)\n",
    "        # protein type-subject type grouped ordered by highest reads first\n",
    "        groupedProteinSubject=df[['Query','SubjectType','ProteinType']].groupby(['SubjectType','ProteinType']).count().sort_values(by=['Query'],ascending=False)\n",
    "\n",
    "        #  Iterate DF with query, subject type and protein type\n",
    "        for index, row in groupedQuerySubject.iterrows():\n",
    "            # get all reads for current subject type\n",
    "            current_reads=df[df.SubjectType == index].Query\n",
    "            # unique reads for current HPV type=intersection of total uniques and current reads\n",
    "            unique_reads_count=len(unique_queries_set.intersection(current_reads))\n",
    "            subjects_reads_unique_full = subjects_reads_unique_full+index+'='+str(unique_reads_count)+';' # construct string\n",
    "             # get exclusive HPV counts for subject type\n",
    "            unique_reads_count_exclusive=exclusive_reads_count(df,index)\n",
    "            subjects_reads_exclusive_full = subjects_reads_exclusive_full+index+'='+str(unique_reads_count_exclusive)+';' # construct string\n",
    "            subject_unique_reads[index] = unique_reads_count # add to dict unique reads count\n",
    "            '''\n",
    "            !! For unique non overlapping (most adundant) HPV counts. !!\n",
    "            !! Shared reads are count only for the one with most reads !!\n",
    "            This is the unique reads for subject type by excluding the reads\n",
    "            counted for previous subject types in the sort order of results.\n",
    "            1. get unique reads by intersection of total unique reads set after the reads of previous subject type are counted\n",
    "            2. remove these reads from the total unique set as they are counted for current subject type\n",
    "            '''\n",
    "            unique_currentReads=uniqueQueries_set_reducing.intersection(current_reads)\n",
    "            uniqueQueries_set_reducing=uniqueQueries_set_reducing.difference(unique_currentReads)\n",
    "            subjects_reads_uniqueNonOverlap_full = subjects_reads_uniqueNonOverlap_full+index+'='+str(len(unique_currentReads))+';'\n",
    "\n",
    "\n",
    "        # Coverage length per subject type\n",
    "        for index,row in groupedProteinSubject.iterrows():\n",
    "            subjectType = index[0]\n",
    "            protein =  index[1]\n",
    "            alignmentDf = df[(df.SubjectType ==  subjectType ) & (df.ProteinType == protein )]\n",
    "            alignmentPositions=alignmentDf[['SA','EA']].values.tolist()\n",
    "            coverageLen_current=count_merged_intervals(alignmentPositions)\n",
    "            subjects_cov_len=subjects_cov_len+index[0]+'|'+index[1]+'='+str(coverageLen_current)+';'\n",
    "            covLenPerSub=coverage_per_sub_type.get(subjectType)\n",
    "            if covLenPerSub:\n",
    "                coverage_per_sub_type[subjectType]=covLenPerSub+coverageLen_current\n",
    "            else :\n",
    "                coverage_per_sub_type[subjectType]=coverageLen_current\n",
    "\n",
    "\n",
    "         # total cov len per subject type\n",
    "        for subType,covLen in coverage_per_sub_type.items():\n",
    "            subject_total_cov_len = subject_total_cov_len+subType+'='+str(covLen)+';'\n",
    "            # check for positive\n",
    "            reads=subject_unique_reads.get(subType)\n",
    "            if not is_positive:\n",
    "                is_positive=check_positive(reads,covLen)\n",
    "\n",
    "\n",
    "        # paired reads count\n",
    "        querySubject=df[['Query','SubjectType']].sort_values(by=['Query'])\n",
    "        queries=querySubject.Query\n",
    "        subjects=querySubject.SubjectType\n",
    "        r1=[]\n",
    "        r2=[]\n",
    "        for count,item in enumerate(queries):\n",
    "            seq=item.split('/')[0]\n",
    "            if '/1' in item:\n",
    "                r1.append((seq,subjects[count]))\n",
    "            elif '/2':\n",
    "                r2.append((seq,subjects[count]))\n",
    "\n",
    "        r1_dict = dict(Counter(r1))\n",
    "        r2_dict = dict(Counter(r2))\n",
    "        pairs=0 # number of unique pairs\n",
    "        for r1_item in r1_dict.items():\n",
    "            r1_key=r1_item[0]\n",
    "            r1_value=r1_item[1]\n",
    "            if r1_key in r2_dict:\n",
    "                r2_value=r2_dict.get(r1_key)\n",
    "                pairs+=min([r1_value,r2_value])\n",
    "\n",
    "        # get original sample name\n",
    "        sample=os.path.splitext(file_name)[0].replace('_NH_unmapped','').replace('Hpv_trim_','').replace('_filtered','')\n",
    "\n",
    "\n",
    "        # add values into header-row map\n",
    "        headerRowData[HDR_FILE_NAME]=file_name\n",
    "        headerRowData[HDR_SAMPLE_NAME]=sample\n",
    "        headerRowData[HDR_TOTAL_READS]=str(totalRows)\n",
    "        headerRowData[HDR_UNIQUE_READS]=str(unique_count)\n",
    "        headerRowData[HDR_UNIQUE_PAIR_ENDS]=str(pairs)\n",
    "        headerRowData[HDR_HPV_COUNT_UNIQUE]=subjects_reads_unique_full\n",
    "        headerRowData[HDR_HPV_COUNT_EXCLUSIVE]=subjects_reads_exclusive_full\n",
    "        headerRowData[HDR_HPV_TYPES_NONOVERLAP]=subjects_reads_uniqueNonOverlap_full\n",
    "        headerRowData[HDR_HPV_TYPE_COV_PAIR]=subjects_cov_len\n",
    "        headerRowData[HDR_COV_PER_SUBTYPE]=subject_total_cov_len\n",
    "        headerRowData[HDR_POVSITIVE]=str(is_positive)\n",
    "\n",
    "        return str.join('\\t', headerRowData.values()) # return row as single string\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def exclusive_reads_count(df,subject_type):\n",
    "    \"\"\"\n",
    "    Calculate unique exclusive reads for HPV type\n",
    "    :param df: DF with subject type and queries\n",
    "    :param subject_type: HPY type\n",
    "    :return: Integer: exclusive unique reads count\n",
    "    \"\"\"\n",
    "    current_reads=set(df[df.SubjectType == subject_type].Query.unique())\n",
    "    other_types_reads=set(df[df.SubjectType != subject_type].Query.unique())\n",
    "    unique_reads_count=len(current_reads.difference(other_types_reads))\n",
    "    return unique_reads_count\n",
    "        \n",
    "\n",
    "def count_merged_intervals(intervals, sort=True):\n",
    "    '''Gets a list of intervals (should be sorted or marked to be sorted with sort=True), \n",
    "       merges them when overlapping and then calculate the sum (including limits).\n",
    "       Original list: [4, 8], [10, 12], [1, 5]\n",
    "       Sorted and merged: [1, 8], [10, 12]\n",
    "       Returns: 11\n",
    "    '''\n",
    "    intervals = np.array(intervals)\n",
    "    if sort:\n",
    "        intervals.sort(axis=0)\n",
    "    starts = intervals[:, 0]\n",
    "    ends = np.maximum.accumulate(intervals[:, 1])\n",
    "    valid = np.zeros(len(intervals) + 1, dtype=np.bool)\n",
    "    valid[0] = True\n",
    "    valid[-1] = True\n",
    "    valid[1:-1] = starts[1:] >= ends[:-1]\n",
    "    merged = np.vstack((starts[:][valid[:-1]], ends[:][valid[1:]])).T\n",
    "    # resultado2 = np.zeros(len(merged), dtype=np.int)\n",
    "    resultado2 = merged[:, 1] - merged[:, 0] + 1\n",
    "    return np.sum(resultado2)\n",
    "\n",
    "\n",
    "def check_positive(reads,coverage,threshold_unique_count=10,threshold_coverage=230):\n",
    "    \"\"\"\n",
    "    Flag sample as positive or negative based on thresholds\n",
    "    :param reads: Reads for given subject type\n",
    "    :param coverage: Coverage length for given subjet type\n",
    "    :param threshold_unique_count: threshold for reads\n",
    "    :param threshold_coverage: threshold for coverage length\n",
    "    :return: True if positive\n",
    "    \"\"\"\n",
    "    if reads >= threshold_unique_count and coverage >= threshold_coverage:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_subject_types(subjects):\n",
    "    \"\"\"\n",
    "    Split Subjects column into corresponding subject types\n",
    "    :param subjects: List\n",
    "    :return: subjects_filtered:List\n",
    "    \"\"\"\n",
    "    splt_char='-'\n",
    "    K=2 # index for splitted array\n",
    "    subjects_filtered=[]\n",
    "    for item in subjects:\n",
    "        sub_grp=item.split('|')[-2]\n",
    "        splits_arr = sub_grp.split(splt_char)\n",
    "        if len(splits_arr)==3: # incase of exceptions e.g. HPV-mSK152\n",
    "            sub = splt_char.join(splits_arr[:K])\n",
    "        else :\n",
    "            sub = splits_arr[0]\n",
    "        subjects_filtered.append(sub)\n",
    "    \n",
    "    return subjects_filtered\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for testing\"\"\"\n",
    "#table2('hdfs:///Projects/TCGA_viruses/Diamond/raw/Hpv_trim_008d4eb5-91d6-4c69-b24b-afe15ff18c17_gdc_realn_rehead_NH_unmapped')\n",
    "#table2('hdfs:///Projects/TCGA_viruses/Diamond/raw/Hpv_trim_004660a1-aaed-4619-87a0-e8260f09128a_gdc_realn_rehead_NH_unmapped')\n",
    "#table2('hdfs:///Projects/TCGA_viruses/Diamond/raw/Hpv_trim_09f31b55-df37-4673-8a8c-bde9bc8780c4_gdc_realn_rehead_NH_unmapped')\n",
    "\n",
    "#all_files=['hdfs:///Projects/TCGA_viruses/Diamond/raw/Hpv_trim_004660a1-aaed-4619-87a0-e8260f09128a_gdc_realn_rehead_NH_unmapped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(all_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9759"
     ]
    }
   ],
   "source": [
    "outputLines=rdd.map(table_collect_stats).collect()\n",
    "len(outputLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started copying local path report_diamond_raw_uniqueHpv.txt to hdfs path hdfs://rpc.namenode.service.consul:8020/Projects/TCGA_viruses/Temp//report_diamond_raw_uniqueHpv.txt\n",
      "\n",
      "Finished copying"
     ]
    }
   ],
   "source": [
    "def writeFile(outputLinesList):\n",
    "\n",
    "    with open(REPORT_FILE, 'w') as f:\n",
    "        headers=str.join('\\t',headerRowData.keys())\n",
    "        f.write(headers)\n",
    "        f.write('\\n')\n",
    "        for i in outputLinesList:\n",
    "            f.write(i+'\\n')\n",
    "\n",
    "\n",
    "    hdfs.copy_to_hdfs(REPORT_FILE,OUTPUT_ROOT,overwrite=True)\n",
    "\n",
    "\n",
    "writeFile(outputLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hpv_trim_0047afe9-2225-4a5d-8cfa-234b1f8ba78c_gdc_realn_rehead_NH_unmapped\\t0047afe9-2225-4a5d-8cfa-234b1f8ba78c_gdc_realn_rehead\\t15\\t13\\t0\\tHPV170=3;HPV-mICB2=2;HPV-mSK065=2;HPV173=2;HPV-mSK037=2;HPV51=1;HPV-mSK100=1;HPV112=1;HPV-mw18c07=1;\\tHPV170=3;HPV-mICB2=2;HPV-mSK065=2;HPV173=0;HPV-mSK037=2;HPV51=1;HPV-mSK100=1;HPV112=1;HPV-mw18c07=1;\\tHPV170|E1=21;HPV-mICB2|L2=21;HPV-mSK037|E1=24;HPV-mSK065|L1=17;HPV173|L1=17;HPV-mSK100|E1^E4=21;HPV-mw18c07|L2=16;HPV112|L2=18;HPV51|E1=16;\\tHPV170=21;HPV-mICB2=21;HPV-mSK037=24;HPV-mSK065=17;HPV173=17;HPV-mSK100=21;HPV-mw18c07=16;HPV112=18;HPV51=16;\\tFalse'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}